import numpy as np
import tensorflow as tf
from keras import layers
from game.players import BasePokerPlayer
import random
from collections import deque

class RLPlayer(BasePokerPlayer):
    def __init__(self):
        super(RLPlayer, self).__init__()
        self.state_size = 10  # Example state size, adjust as needed
        self.action_size = 3  # Fold, Call, Raise
        self.memory = deque(maxlen=2000)
        self.gamma = 0.95  # discount rate
        self.epsilon = 1.0  # exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.model = self._build_model()

    def _build_model(self):
        model = tf.keras.Sequential()
        model.add(layers.Dense(24, input_dim=self.state_size, activation='relu'))
        model.add(layers.Dense(24, activation='relu'))
        model.add(layers.Dense(self.action_size, activation='linear'))
        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))
        return model

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state, valid_actions):
        if np.random.rand() <= self.epsilon:
            return random.choice(valid_actions)["action"], 0  # Randomly choose an action
        act_values = self.model.predict(state)
        valid_act_values = [act_values[0][i] for i in range(len(valid_actions))]
        best_action = valid_actions[np.argmax(valid_act_values)]
        return best_action["action"], best_action.get("amount", 0)

    def replay(self, batch_size):
        minibatch = random.sample(self.memory, batch_size)
        for state, action, reward, next_state, done in minibatch:
            target = reward
            if not done:
                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))
            target_f = self.model.predict(state)
            target_f[0][self._action_to_index(action)] = target
            self.model.fit(state, target_f, epochs=1, verbose=0)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def _action_to_index(self, action):
        if action == "fold":
            return 0
        elif action == "call":
            return 1
        elif action == "raise":
            return 2

    def _encode_cards(self, hole_card):
        # Convert cards to a numerical value for simplicity
        value_dict = {'2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, 'T': 10, 'J': 11, 'Q': 12, 'K': 13, 'A': 14}
        suit_dict = {'S': 0, 'H': 1, 'D': 2, 'C': 3}
        card_values = [value_dict[card[1]] for card in hole_card]
        card_suits = [suit_dict[card[0]] for card in hole_card]
        return card_values + card_suits

    def get_state(self, valid_actions, hole_card, round_state):
        state = np.zeros(self.state_size)
        encoded_cards = self._encode_cards(hole_card)
        state[:4] = encoded_cards
        # Additional state information from round_state can be added here
        return state

    def declare_action(self, valid_actions, hole_card, round_state):
        state = self.get_state(valid_actions, hole_card, round_state)
        state = np.reshape(state, [1, self.state_size])
        action, amount = self.act(state, valid_actions)
        return action, amount

    def receive_game_start_message(self, game_info):
        pass

    def receive_round_start_message(self, round_count, hole_card, seats):
        pass

    def receive_street_start_message(self, street, round_state):
        pass

    def receive_game_update_message(self, action, round_state):
        pass

    def receive_round_result_message(self, winners, hand_info, round_state):
        pass

def setup_ai():
    return RLPlayer()
